spark是一个计算框架，分为spark-core，saprk-sql，spark-streaming，spark-Elib，spark-graphX。
可以看做是Hadoop的一个改良框架。相比于handoop的硬盘计算，spark使用内存计算使其更快。但是因为这一点，在集群计算时可能会抢占更多资源导致不稳定，所以spark不能完全代替hadoop  
一个比较详细的配置帮助：https://www.cnblogs.com/xuliangxing/p/7279662.html
