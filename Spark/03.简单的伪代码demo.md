#### 以下是总结的一个简单的spark分析的伪代码demo
```java
import cn.hutool.core.bean.BeanUtil;
import lombok.extern.slf4j.Slf4j;
import org.apache.commons.lang3.StringUtils;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import scala.Tuple2;

import java.sql.Timestamp;
import java.util.Calendar;
import java.util.Date;
import java.util.List;

/**
 * @author: wangcong
 * @version: 1.0
 * @description: spark demo
 * @date:
 */
@Slf4j
public class Demo {
    public static void main(String[] args) {

        //参数在args里面
        log.info("首先处理传参args[]");
        //定义一个标识符
        SparkUtils sparkUtils = new SparkUtils("application.id");

        try{
            //这里获取配置文件中的sql，工具类在我另一篇里面有些
            String sql = SparkPropertiesUtil.getProp("xxx.xxx.sql");
            //根据sql查询原始数据
            Dataset<Row> dataset = sparkUtils.getDataset(sql);
            //打印原始数据(生产上不要这个)
            dataset.show();
            //构建JavaRDD，这里的OriginalRecordBean是原始表的bean，这一步是吧查询到的数据Dataset转换为JavaRDD
            JavaRDD<OriginalRecordBean> originalRecordJavaRDDMap = (JavaRDD<OriginalRecordBean>) dataset.javaRDD().map(r -> {
                //里面是做一些列的转换
                OriginalRecordBean orb = new OriginalRecordBean();
                orb.setUserName(r.getAs("user_name"));
                orb.setCompany(r.getAs("company"));
                orb.setUrl(r.getAs("url"));
                orb.setNumber(0);
                
                return orb;
            });

            //根据公司和url进行分组，作为key，数据作为value
            JavaPairRDD<String, OriginalRecordBean> javaPairRDD = (JavaPairRDD<String, OriginalRecordBean>)originalRecordJavaRDDMap.mapToPair(vr -> {
                return new Tuple2<String, OriginalRecordBean>(vr.setCompany() + vr.setUrl(), vr);
            });

            //组成数据，combineByKey有三个函数，第一个组件结构，第二个在不同分区处理数据，第三个合并分区
            JavaRDD<ResultBean> ResultBeanMap = javaPairRDD.combineByKey(
                    new Function<OriginalRecordBean, TempBean>() {
                        @Override
                        public TempBean call(OriginalRecordBean originalBean) throws Exception {
                            TempBean tempBean = new TempBean();

                            tempBean.setUserName(originalBean.getUserName());
                            tempBean.setCompany(originalBean.getCompany());
                            tempBean.setUrl(originalBean.getUrl());

                            return tempBean;
                        }
                    },
                    new Function2<TempBean, OriginalRecordBean, TempBean>() {
                        @Override
                        public TempBean call(TempBean tempBean, OriginalRecordBean originalBean) throws Exception {
                           
                            //这里处理累加等各种操作

                            return tempBean;
                        }
                    },
                    new Function2<TempBean, TempBean, TempBean>() {
                        @Override
                        public TempBean call(TempBean tempBean1, TempBean tempBean2) throws Exception {
                            //分区合并
                            tempBean1.addNumber(tempBean2.getNumber());


                            return tempBean1;
                        }
                    }
            ).map(vr -> {
                TempBean tb = vr._2;
                ResultBean result = new ResultBean();
                BeanUtil.copyProperties(tb, result);
                //这里可以再写一些上面没有处理的逻辑
                // todo
                
                return result;
            });


            //写入数据
            sparkUtils.write(ResultBeanMap.rdd(), ResultBean.class, SparkPropertiesUtil.getProp("table.name"));
            
        } catch (Exception e) {
            log.error("exception:",e);
        } finally {
            sparkUtils.closeSparkSession();
        }
    }
}

```
